# Problems

1. 训练到一定程度时继续训练loss不减反增
2. 按道理adam不用太在意learning rate，但是这里对 learning rate 非常敏感
3. learning rate 很大的时候，loss 迅速下降然后再也不动了